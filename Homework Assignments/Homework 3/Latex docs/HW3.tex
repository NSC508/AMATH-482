\documentclass[11pt]{amsart}

% Standard letter size paper with 1inch margins
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{float}
% Useful packages 
\usepackage{amsmath, amssymb, amsthm, amsaddr}
\usepackage{enumerate, subcaption, graphicx, hyperref}


\title{AMATH 482/582: Homework 2}
\author{Sathvik Chinta} % first and last name

\date{\today} % you can also just type the date instead of "\today"

\begin{document}

\maketitle 

\begin{abstract}
     
    % Your report should contain a brief, 100 word abstract describing what is contained in 
    % the document and what you did. {\bf Don't forget 6 pages max}.
\end{abstract}


\section{Introduction and Overview}\label{sec:Introduction}
This time we are given data from a winery in Portugal. We are given a series of chemical measurements and the corresponding quality of the wine 
in our data. 
% Here you will give a brief introduction to the problem you solved. Including 
% some discussion of relevant literature and background. 

% Make sure you use the correct citation commands (i.e., \texttt{$\backslash$cite}) to keys 
% from your bib file like this \cite{example-article-citation}. If you want 
% to cite more than one reference simply use \cite{example-article-citation, example-book-citation}. You can grab latex citations 
% from \href{https://scholar.google.com}{Google Scholar}. Just keep in mind that they often 
% need to be cleaned up.

\section{Theoretical Background}\label{sec:theory}
We will firstly be using a simple linear regression model to predict the quality of the wine. This is very similar 
to our last problem, where we used linear regression to classify handwritten digits.

However, we will be using a more complex models as well since we will find that linear regression's error is very high on 
both training and test data. We will use Kernel Ridge Regression in order to do so. Ridge Regression can be defined 
as 

\[minimize_{f \in H_k} ||f(x) - Y||^2 + \lambda||f||^2_{H_k}\]

Where $H_k$ is the kernel function induced by features that map from a high
dimensional space to $\mathbb{R}$. In our case, we will use Gaussian (RBF) and Laplacian Kernel functions 
in order to model our data. The Gaussian Kernel function can be defined as 

\[exp(-\frac{||x  - x'||^2_2}{2\sigma^2})\]

And Laplacian can be similarly defined as 

\[exp(-\frac{||x -  x'||_1}{\sigma})\]

Notice that we no longer have $Y$ in our equations, and isntead we have sigma. This is simply 
because of notation, we defined $\sigma = \sqrt{2Y}$. As such, we need to find the ideal 
values of both $\sigma$ and $\lambda$. We will use the following procedure to find the ideal values:

Take a 10-fold cross validation set. In cross validation, we split the training data and use some pieces for training and other 
for testing while we find our ideal values for hyperparameters. 

In our case for each fold, we will check the negative mean squared error of 
the model on the training data, until we find the parameters that output the best results. We will then use these 
parameters for our model. If either one of the parameters is equal to the endpoints of either one of our parameter's 
ranges, we move our range in order to encapsulate the best results. We will also use 10 values within the search range 
for each parameter, effectively giving us a 10 x 10 search grid. 

With those parameters figured out, we will use them in order to train our model on the training data. We will then check how well they perform 
against the test dataset. 
% You dedicate this section to the theoretical background of the methods and frameworks 
% that you used in your homework. This is not meant to reproduce material from the lectures
%  or references you used but rather to demonstrate your understanding of the 
%  mathematical foundations of the methods and algorithms. You can create equations like this 
%  \begin{equation*}
%      f(x) = \int_A \sin( \pi x) dx.
%  \end{equation*}
%  You do not need to label your equations unless they are referenced in the text. In that 
%  case simply use 
%  \begin{equation}\label{eq:meaningful-label}
%       - \frac{\partial^2 u}{\partial x^2} = \sin ( \pi x).
%  \end{equation}
% Also look up the \texttt{align} or \texttt{aligned} environments if you want multi-line 
% equations. You can then reference your equations in text using the $\backslash$\texttt{eqref}
% command as such \eqref{eq:meaningful-label}. 

\section{Algorithm Implementation and Development}\label{sec:algorithms}


% Here you discuss the algorithms and software packages that you used. Not much to it. 
% Just make sure you cite the packages properly and avoid including code. 
% You are welcome to use \LaTeX packages that are specifically designed to show 
% algorithms such \href{https://www.overleaf.com/learn/latex/Algorithms}{as this}, but it is 
% not always worth the effort and real estate. 


\section{Computational Results}\label{sec:results}
After doing the normalization of the training data along with the linear regression, I got the following results 

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c |}
         \hline
         LINEAR REGRESSION TRAINING MSE & LINEAR REGRESSION TEST MSE\\ \hline
         0.6278484956554882 & 0.747169690518721 \\ \hline
    \end{tabular}
    \caption{Mean squared error of Linear Regression}
    \label{tab:LinearRegressionMSE}
\end{table}

We can see that the error is pretty high in both cases. 

Looking at the RBF Kernel, I got the following hyperparameters as optimal

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c | c | c |}
         \hline
         $\alpha$ & $\gamma$ & $\lambda$ & $\sigma$ \\ \hline
         0.19842513149602492 & 0.03645403248675365 & -2.33333 & 1.888889\\ \hline
    \end{tabular}
    \caption{Optimal hyperparameters for RBF Regression}
    \label{tab:OptRBF}
\end{table}

This resulted in the following MSE

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c |}
         \hline
         GAUSSIAN KERNEL TRAINING MSE & GAUSSIAN KERNEL TEST MSE\\ \hline
         0.4443644835200732 & 0.6707146529560462 \\ \hline
    \end{tabular}
    \caption{Mean squared error of Gaussian Kernel}
    \label{tab:GaussianKernelMSE}
\end{table}

Looking at the error values, it's clear that we are performing better than the Linear Regression model. Both 
training and test error values are lower than before, with a higher percentage decrease for the training error. Let's see 
if we can make these results even better with the Laplacian Kernel 

I got the following hyperparameteres as optimal for the Laplacian Kernel 

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c | c | c |}
         \hline
         $\alpha$ & $\gamma$ & $\lambda$ & $\sigma$ \\ \hline
         0.2143109957132682 & 0.21431099571326834 & -2.2222222 & 0.6111111\\ \hline
    \end{tabular}
    \caption{Optimal hyperparameters for Laplacian Regression}
    \label{tab:OptLap}
\end{table}

With these parameters, I got the following MSE 

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c |}
         \hline
         LAPLACIAN KERNEL TRAINING MSE & LAPLACIAN KERNEL TEST MSE\\ \hline
         0.058153392810602604 & 0.6095835762585938 \\ \hline
    \end{tabular}
    \caption{Mean squared error of Gaussian Kernel}
    \label{tab:LaplacianKernelMSE}
\end{table}

Both errors decreased, but the training error decreased by a lot! Even though we are only about 39\% accurate 
on the test data, we are about 95\% accurate on the training! This is a clear sign of overfitting, but it still looks like 
this model performs the best on the test data out of all the models. 

So, putting them all together, here are the results: 

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c | c |}
         \hline
         & TRAINING & TESTING\\ \hline
         LINEAR REGRESSION & 0.6278484956554882 & 0.747169690518721 \\ \hline
         GAUSSIAN & 0.4443644835200732 & 0.6707146529560462 \\ \hline
         LAPLACIAN & 0.058153392810602604 & 0.6095835762585938 \\ \hline
    \end{tabular}
    \caption{Mean squared error of all methods}
    \label{tab:All MSE}
\end{table}


On the new wine data, the results are as follows:
\begin{table}[H]
    \centering
    \begin{tabular}{| c | c |}
         \hline
         & PREDICTION \\ \hline
         LINEAR PREDICTION & [6.00469789 5.28767761 5.56363072 6.067022   5.94248207]  \\ \hline
         RBF PREDICTION & [6.02506681 5.45131357 5.42488008 6.18782517 6.1157892 ]  \\ \hline
         LAPLACIAN PREDICTION & [6.06872311 5.45686107 5.63144745 6.00113834 6.03145372]  \\ \hline
    \end{tabular}
    \caption{Raw predictions on new wine data for all models}
    \label{tab:RawPred}
\end{table}

We are not getting integers in our predictions since we still have some uncertainty no matter how good our model is. 
This is becasue of the confidence of our model. If we were to round these results to the nearest integer, they would look like so

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c |}
         \hline
         & PREDICTION \\ \hline
         LINEAR PREDICTION & [6 5 6 6 6]  \\ \hline
         RBF PREDICTION & [6 5 5 6 6]  \\ \hline
         LAPLACIAN PREDICTION & [6 5 6 6 6]  \\ \hline
    \end{tabular}
    \caption{Raw predictions on new wine data for all models}
    \label{tab:RoundPred}
\end{table}

An interesting observation is that the predictions for Linear and Laplacian are the exact same.

%This is perhaps the most important section of your report. You want to dedicate more space 
% here and present your numerical results in a clear, concise and meaningful way. Also 
% include a discussion of your numerics. Think hard about how you can use 
% your space most efficiently. For example, include subplots and multiple error curves on the 
% same plot etc. Ask us for advice when the time comes. 

% You will most definitely need tables and figures. So here is an example. 

% \begin{table}[htp]
%     \centering
%     \begin{tabular}{| l | c|c | r |}
%          \hline
%          row 1 & column 1  & column 2  \\ \hline
%          row 2 & column 1 & column 2 \\ 
%          row 3 & column 1 & column 2 \\ \hline
%     \end{tabular}
%     \caption{Don't forget to include a caption for your table. Say a few words about what is 
%     being shown.}
%     \label{tab:meaningful-label}
% \end{table}

% Make sure your table is labeled and referenced withing the text using $\backslash$\texttt{ref} as such Table~\ref{tab:meaningful-label}. In fact, you can 
% use $\backslash$\texttt{ref} to cite anything else in the document such as 
% sections (ex. Section~\ref{sec:Introduction}). This will create hyperlinks in your 
% pdf after compilation and automatically update the numbers and tags whenever you change 
% anything. 

% Figures are very similar to tables. Here's an example: 

% % \begin{figure}[htp]
% %     \centering
% %     \includegraphics[width=0.4\textwidth]{./Figs/fig1.pdf}
% %     \caption{Include a descriptive caption for your figure. Also make sure all 
% %     legends, axis labels, and titles are large enough to be readable. You might have 
% %     to reproduce the plots from Python or MATLAB with larger fonts for this purpose. It 
% %     can be annoying the first time you do it but it is crucial.}
% %     \label{fig:meaningful-label}
% % \end{figure}

% You may also need to include multiple figures: 

% % \begin{figure}
% %     \centering
% %     \begin{subfigure}[b]{.3\textwidth}
% %     \includegraphics[width=\textwidth]{./Figs/fig1.pdf}
% %     \caption{First subfigure}
% %     \label{subfig:first}
% %     \end{subfigure}
% %     \begin{subfigure}[b]{.3\textwidth}
% %     \includegraphics[width=\textwidth]{./Figs/fig2.pdf}
% %     \caption{First subfigure}
% %     \label{subfig:second}
% %     \end{subfigure}
% %     \begin{subfigure}[b]{.3\textwidth}
% %     \includegraphics[width=\textwidth]{./Figs/fig3.pdf}
% %     \caption{First subfigure}
% %     \label{subfig:third}
% %     \end{subfigure}
% %     \caption{Caption for entire figure. You don't need to use captions for subfigs so 
% %     feel free to eliminate the subcaption texts to just have the A, B, C labels.}
% %     \label{fig:meaningful-label-2}
% % \end{figure}

% Once again, make sure all your figures are referenced like Figure~\ref{fig:meaningful-label}
% or Figure~\ref{subfig:first} in the text body of the report and discussed 
% in detail. This is where you will make observations about your results and we will 
% look at these very closely. 

% Also note, I am using PDF figures. These give you the best looking graphs but PNG works 
% well too. I advise staying away from JPG as it always looks weird and low quality.]
% Both Python and MATLAB can output figures in PDF or PNG.

\section{Summary and Conclusions}\label{sec:conclusions}

% Wrap up your report with a brief summary of what you did and what you discovered. 
% Finish with some conclusions and possibly future directions if any. 

\section*{Acknowledgements}

% Make sure you you clearly state any help you received including collaborations 
% with your peers. Help from TAs or other mentors, professors, etc that helped you 
% with your assignment. Here's a formal example: 

% The author is thankful to Prof. X for useful discussions about the QR algorithm. 
% We are also thankful to Dr. Strange for suggesting the JAX software package for 
% automatic differentiation. Furthermore, our peer Jean Grey was helpful in 
% implementation of spectral clustering in Python.

 % make sure this matches the .bib file for your corresponding document. You also have to maintain your references in the .bib file 
\end{document}
